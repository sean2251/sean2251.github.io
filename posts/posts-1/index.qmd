---
title: "Building EPA: Starting with Expected Points"
description: "Introducing the concept of expected points and building some models."
author: "Sean McSweeney"
date: "2024-05-24"
categories: [R, Quarto, Analysis]
---

Ultimate Frisbee statistics have advanced significantly beyond purely scores, assists, and turnovers. Most prominently, [Ultiworld's EDGE](https://ultiworld.com/tag/edge/) statistic adds more relevant in game context to the standard scores, assists, and turnovers by incorporating that game's number of turnovers and total yards thrown. In other words, we should rate a player higher when they throw 3 turnovers in a game of 25 total turnovers versus a game of 15 total turnovers. While a big improvement, there still is a lot more in game context that we can add. In this project I will be creating a new statistic based on a logistic regression model called Expected Points Added (EPA). This football inspired statistic should have strong intuitive appeal: If I catch the pull around my own endzone, there's a certain probability that we'll score that possession, say 60%. Conveniently, in Ultimate there's only one way to score and each score is worth exactly one point so we can translate that percentage into an Expected Point (EP) value of 0.6. Now let's say I throw a 30 yard huck placing us much closer to the scoring endzone. At this point of the field there's say a 75% chance of score, or an EP of 0.75. We can then quantify my throw as having an Expected Point Added value of 0.75-0.6 = 0.15.

The intuition remains the same for turnovers. Consider two examples. One, if I throw a turnover on that huck, then maybe the probability of us scoring that point is now 30%, leading to an EPA of 0.3-0.6=-0.3. Two, if I had instead turned it over on a reset throw, it's now much easier for the other team to score so the chance of us getting it back and scoring is much lower at 5%. So an EPA of 0.05-0.6=-0.55.

In this post I'll go over the first step of this process by creating the EP model. That is, given where the disc is on the field and the in-game situation, what's the probability of scoring on that possession?

#### The data

Data comes from RAMP (a regionals level mixed club team) games from 2022-2024 recorded with the Statto app. Information on player role (cutter, handler, or hybrid is manually entered). There was a fair amount of processing of the data that needed to be done. I'm planning to create an easy to use version of this where anyone can plug in their own Statto generated files and get similar results.

#### Why logistic regression?

I explored a number of other different options but decided on logistic regression for a number of reasons. One, the outcome we're interested in, score or not, is a binary variable. Two, we're interested in the probability of said event which is easy to calculate from a logistic regression. Three, it's more intuitive to grasp than a non-parametric model (I did try different non-parametric models like random forests but found them to about the same level of accuracy with a big loss in explainability). Four, while I'll ultimately use a more complicated multiple logistic regression model that doesn't really lend itself to visualization, the more simple models I'll start off with do which hopefully will help start developing the intuition.

#### The Starting Point: Vertical Field Position

Conventional wisdom is that the most important variable in determining the likelihood of scoring on any given possession is vertical field position. That is, the closer a team is to the scoring endzone the higher likelihood of scoring. From that, I built a basic single logistic regression with field position as the sole independent variable.

$$ \operatorname{logit}(P(\text{Scored})) = \beta_0 + \beta_1(\text{pass.start.y.yards}) $$

```{r}
#| echo: false
#| warning: false

library(tidyverse)
library(caret)
library(gtsummary)
library(gt)

# make the data for all years, 
all_points_22_24 <- read_csv("posts/posts-1/final_csvs/all_ramp_points.csv")
all_passes_22_24 <- read_csv("posts/posts-1/final_csvs/all_ramp_passes.csv")

# get the total number of our turnovers and opp_turnovers per game
turnovers_per_game_22_24 <- all_points_22_24 |>
  drop_na(game) |>
  mutate(our_turns = Turnovers,
         opp_turns = `Defensive blocks` + `Opposition errors`) |>
  group_by(game) |>
  summarise(our_turns_total = sum(our_turns),
            opp_turns_total = sum(opp_turns),
            our_possessions_total = sum(Possessions),
            our_passes_total = sum(Passes)) |>
  ungroup() |>
  mutate(our_turns_per_poss = our_turns_total / our_possessions_total,
         our_turns_per_pass = our_turns_total / our_passes_total)

game_stats_22_24 <- all_passes_22_24 |> 
  drop_na(game) |>
  mutate(`Scored?` = as.factor(`Scored?`),
         `Opp_scored?` = as.factor(ifelse(`Scored?` == 1, 0, 1))) |>
  group_by(game) |>
  mutate(total_turnovers = sum(`Turnover?`),
         total_yards_thrown = sum(`Forward distance (m)`)) |>
  ungroup() |>
  left_join(turnovers_per_game_22_24)

games <- unique(game_stats_22_24$game)


# very simple EPA model logistic model
simple_epa_logistic <- glm(`Scored?` ~ pass.start.y.yards, data = game_stats_22_24, family = binomial)

tbl_regression(simple_epa_logistic, exponentiate = T, label = list(pass.start.y.yards = "Pass start y-distance (yd)")) |> 
add_significance_stars() |>
as_gt() |>
gt::tab_header(title = "Table 1: EP Logistic Regression Model")
```

The key takeaway from here is that every additional yard closer to the endzone a pass starts increases the odds of a score that possession by 2%. Or put another way, every 10 yards closer to the endzone results in the odds of scoring increasing by about 20%. This value is both large and statistically significant which fits with our initial intuitions about field location and prior research.

We can see visualize this result by plotting different field positions vs the probability of scoring in each of those points here. I've included the actual passes as the circles as well in order to get a sense of what the actual data are and where the model comes from. Also note the paucity of passes at the extremes (i.e. closest to 0 yards and 90 yards) which we will see later leads to a more poorly calibrated model at those extreme points.

```{r}
#| echo: false
#| warning: false
#| fig-cap: "Figure 1: Expected Points (EP) Model"

# 1. Create a prediction data frame for a smooth line and predict probabilities
simple_epa_plot_data <- data.frame(pass.start.y.yards = seq(0,110,length.out = 110))

# 2. Predict probabilities using your model
simple_epa_plot_data$prob <- predict(simple_epa_logistic, newdata = simple_epa_plot_data, type = "response")

# 2b. Create Binned Data to show actual scoring rates
# We group by 5-yard increments and calculate the mean of 'Scored?'
binned_data <- game_stats_22_24 %>%
  mutate(yard_bin = round(pass.start.y.yards / 5) * 5) %>%
  group_by(yard_bin) %>%
  summarise(
    obs_prob = mean(as.numeric(as.character(`Scored?`)), na.rm = TRUE),
    count = n()
  )

# 3. Create the plot
ggplot(simple_epa_plot_data, aes(x = pass.start.y.yards, y = prob)) +
  # Add the curve
  geom_line(color = "#2c3e50", size = 1.2) +
  # Vertical lines for the Goal Lines (End Zones are 20 yards deep)e
  geom_vline(xintercept = c(20, 90), linetype = "dashed", color = "red", alpha = 0.5) +
  # Labels and scales
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  scale_x_continuous(breaks = seq(0, 110, by = 10)) +
  labs(
    subtitle = "Line is logistic model. Dots are actual passes, sized by count",
    x = "Yardage (0 = Own Backline, 110 = Opponent Backline, red lines = goal line)",
    y = "Probability of Scoring"
  ) +
  theme_minimal() +
  geom_point(data = binned_data, aes(x = yard_bin, y = obs_prob, size = count), 
             color = "black", alpha = 0.7) 

```

At this point it would be helpful to have a metric to 'grade' our EP model by so that as we add complexity we can see if we're improving it. In an ideal world I'd create the model using all my existing games, then play a new game and see if that model is able to accurately predict what happens. While this counterfactual doesn't exist, I can do the next best thing by using leave-one-game-out cross-validation (LOGO CV). This takes my set of 46 games, removes one of them (my Test data), then uses the remaining 45 games as the Training data to create the logistic regression model. Each model is then tested against the Test game (i.e. the left out game) and the results saved. The end result is that now for every pass in the dataset I have the probability that it will result in a possession with a score. With these probabilities in hand I can then bin those probabilities and compare them to what actually happened. That is, possessions that the model predicts should happen 5% of the time should happen 5% of the time if the model. Plotting this out creates a Calibration Plot which allows us to see if our model is working and well-calibrated. Visualizing this also allows us to see if the model remains well calibrated at the extremes, a common issue with logistic regression.

```{r}
#| echo: false
#| include: false

# function takes in a game then runs a LOGO model (the selected game is test data, rest are training data)
game2CV_simple <- function(x) {
  test_data <- game_stats_22_24 %>%
    filter(game == x) %>%
    select(-game)
  train_data <- game_stats_22_24 %>%
    filter(game != x) %>%
    select(-game)
  
  local_ep_model <- glm(`Scored?` ~ pass.start.y.yards , data = train_data, family = binomial)

  preds <- predict(local_ep_model, test_data, type = "response") |>
    bind_cols(test_data) |> 
    mutate(predicted_score = as.factor(ifelse(`...1` > 0.5, 1, 0)),
           game = x, .before = 1) |>
    rename(predicted_score_prob = `...1`)
  return(preds)
}

# do the LOGO over all games
cv_results_22_24_simple <- map_dfr(games, game2CV_simple) 

# calibration plots
calibration_data_22_24_simple <- cv_results_22_24_simple |>
  mutate(bin_pred_prob = round(predicted_score_prob / 0.05) * .05, .before =1) |>
  group_by(`Scored?`, bin_pred_prob) %>%
  mutate(correct = if_else(`Scored?` == 1, 1, 0), .before = 1) |>
  ungroup(`Scored?`) |>
  summarize(
    n_throws = n(),
    n_outcome = sum(correct),
    bin_actual_prob = n_outcome / n_throws
  )


# calibration error
cv_cal_error_simple <- calibration_data_22_24_simple |>
  ungroup() |>
  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) |>
  summarize(
    weight_cal_error = weighted.mean(cal_diff, n_throws, na.rm = TRUE),
    n_scoring_event = sum(n_outcome, na.rm = TRUE)
  )


```

```{r}
#| echo: false
#| fig-cap: "Figure 2: Calibration plot for simple logistic regression"


ann_text <- data.frame(
  x = c(.25, 0.75), y = c(0.75, 0.25),
  lab = c("More times\nthan expected", "Fewer times\nthan expected")
  )

calibration_data_22_24_simple |>
  ungroup() |>
  ggplot() + 
  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_throws)) + 
  geom_abline(slope = 1, intercept = 0, color = "black", lty = 2) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    size = "Number of throws",
    x = "Estimated next score probability",
    y = "Observed next score probability"
  ) + 
  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 5) + 
  theme_bw()

```

Additionally, with this calibration data I can now quantify in one number the level of calibration as well by finding calibration error rate which is a weighted average of how far each observed probability is from its predicted. Here the calculated calibration error is `r round(cv_cal_error_simple$weight_cal_error, digits = 4)`.

#### Adding Complexity

Next we'll increase the complexity of our model by adding in additional independent variables that will hopefully increase the accuracy of our model. These are horizontal field position, each team's turnovers per that game, the thrower gender, receiver gender, thrower role (handler, cutter, hybrid), receiver role, if it's a defensive or offensive point, and if it's a 4 or 3 WMP point.

$$
\begin{aligned}
\operatorname{logit}(P(\text{Scored})) = \beta_0 &+ \beta_1(\text{pass.start.x.yards}) \\
&+ \beta_2(\text{pass.start.y.yards}) \\
&+ \beta_3(\text{our\_turns\_total}) \\
&+ \beta_4(\text{opp\_turns\_total}) \\
&+ \beta_5\mathbb{I}(\text{Thrower\_gender} = \text{WMP}) \\
&+ \beta_6\mathbb{I}(\text{Thrower\_role} = \text{handler}) \\
&+ \beta_7\mathbb{I}(\text{Thrower\_role} = \text{hybrid}) \\
&+ \beta_8\mathbb{I}(\text{Thrower\_line} = \text{oline}) \\
&+ \beta_9\mathbb{I}(\text{gender\_ratio} = \text{4MM}) \\
&+ \beta_{10}\mathbb{I}(\text{line} = \text{oline})
\end{aligned}
$$

```{r}
#| echo: false

#1. Complex model
full_ep_logistic_model <- glm(`Scored?` ~ pass.start.x.yards + 
                   pass.start.y.yards + 
                   our_turns_total + 
                   opp_turns_total + Thrower_gender + Thrower_role + 
                     Thrower_line + gender_ratio + line,
                   data = game_stats_22_24, family = binomial)

tbl_regression(full_ep_logistic_model, exponentiate = T,label = list(pass.start.x.yards = "Pass start x-distance (yd)", pass.start.y.yards = "Pass start y-distance (yd)", our_turns_total = "Our total turns", opp_turns_total = "Opponent total turns", Thrower_gender = "Thrower's gender", Thrower_role = "Thrower's role", Thrower_line = "Thrower's typical line", gender_ratio = "Line gender ratio", line = "Actual line")) |>
  bold_labels() |>
  italicize_labels() |>
add_significance_stars() |>
   as_gt() |>
gt::tab_header(title = "Table 2: EP Logistic Regression Model Full Context")
```

A couple interesting points here. Pass start y-distance keeps the approximately same effect size. In other words, even accounting for all our other contextual factors, getting 10 yards closer still increases the odds of a score by about 20%. O-line is both statistically significant and has a large effect size (i.e. the point being an offensive point increases the odds of scoring by about 93% as compared to a defensive point). Turnovers were statistically significant and had a moderate effect size (i.e. every additional cumulative turnover we committed reduces the odds of scoring by about 10% and every additional cumulative turnover the opponent commits increases the odds of scoring by about 5%). Thrower's role has a minor impact. Handlers and especially hybrids increase scoring probability (note in my dataset there's a relatively small number of hybrids). Gender of thrower and composition of the line (4MMP vs 3MMP) were statistically insignificant. Horizontal field position was also statistically insignificant which was surprising because often conventional wisdom emphasizes the importance of the disc being in the middle of the field.

As before, the calibration plot:

```{r}
#| echo: false
#| include: false
#| warning: false

# function takes in a game then runs a LOGO model (the selected game is test data, rest are training data)
game2CV <- function(x) {
  test_data <- game_stats_22_24 %>%
    filter(game == x) %>%
    select(-game)
  train_data <- game_stats_22_24 %>%
    filter(game != x) %>%
    select(-game)
  
  local_ep_model <- glm(`Scored?` ~ pass.start.x.yards + 
                   pass.start.y.yards + 
                   our_turns_total + 
                   opp_turns_total + Thrower_gender + Thrower_role + 
                     Thrower_line + gender_ratio + line, data = train_data, family = binomial)

  preds <- predict(local_ep_model, test_data, type = "response") |>
    bind_cols(test_data) |> 
    mutate(predicted_score = as.factor(ifelse(`...1` > 0.5, 1, 0)),
           game = x, .before = 1) |>
    rename(predicted_score_prob = `...1`)
  return(preds)
}

# do the LOGO over all games
cv_results_22_24 <- map_dfr(games, game2CV) 

# calibration plots
calibration_data_22_24 <- cv_results_22_24 |>
  mutate(bin_pred_prob = round(predicted_score_prob / 0.05) * .05, .before =1) |>
  group_by(`Scored?`, bin_pred_prob) %>%
  mutate(correct = if_else(`Scored?` == 1, 1, 0), .before = 1) |>
  ungroup(`Scored?`) |>
  summarize(
    n_throws = n(),
    n_outcome = sum(correct),
    bin_actual_prob = n_outcome / n_throws
  )

ann_text <- data.frame(
  x = c(.25, 0.75), y = c(0.75, 0.25),
  lab = c("More times\nthan expected", "Fewer times\nthan expected")
  )


# calibration error
cv_cal_error <- calibration_data_22_24 |>
  ungroup() |>
  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) |>
  summarize(
    weight_cal_error = weighted.mean(cal_diff, n_throws, na.rm = TRUE),
    n_scoring_event = sum(n_outcome, na.rm = TRUE)
  )

```

```{r}
#| echo: false
#| warning: false
#| fig-cap: "Figure 3: Calibration plot for complex logistic regression"

calibration_data_22_24 |>
  ungroup() |>
  ggplot() + 
  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_throws)) + 
  geom_abline(slope = 1, intercept = 0, color = "black", lty = 2) +
  coord_equal() +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    size = "Number of throws",
    x = "Estimated next score probability",
    y = "Observed next score probability"
  ) + 
  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 5) + 
  theme_bw()

```

The calibration error from this model is: `r round(cv_cal_error$weight_cal_error, digits = 4)`. Takeaways from this graph compared to the initial calibration plot is that situational model accounts for a lot more probabilities that are closer to 0% and 100%. This is an improvement because we want our model to be more accurately predicting the scoring probabilities across the spectrum. One note here is the model becomes most inaccurate at the most extremes where the data is most sparse. While this is a normal feature of these kinds of models it's worth noting that in the extremes, times where it's most likely to score or not score, our model is least helpful. There are ways to improve this, called Model Calibration, which I may explore in the future as well.

#### Comparing in-game situations

One way to gain more clarity with regards to the models is to plug in some easily recognizable stock situations and see how the models respond. Additionally, this is a way to see how this model can have value for specific teams using their own datasets in the future. Here I've come up with four different example situations and compared the probability of scoring that possession between the two models.

```{r}
#| echo: false

# 4 stock situations
ex_a <- data.frame(pass.start.x.yards = 20, pass.start.y.yards = 40, our_turns_total = 15, opp_turns_total = 15,
Thrower_gender = 'WMP', Thrower_role = 'handler', Thrower_line = 'oline', gender_ratio = '4MM', line = 'oline')
ex_b <- data.frame(pass.start.x.yards = 20, pass.start.y.yards = 89, our_turns_total = 15, opp_turns_total = 15,
Thrower_gender = 'MMP', Thrower_role = 'cutter', Thrower_line = 'oline', gender_ratio = '4MM', line = 'oline')
ex_c <- data.frame(pass.start.x.yards = 5, pass.start.y.yards = 21, our_turns_total = 15, opp_turns_total = 15,
Thrower_gender = 'WMP', Thrower_role = 'cutter', Thrower_line = 'dline', gender_ratio = '4MM', line = 'dline')
ex_d <- data.frame(pass.start.x.yards = 20, pass.start.y.yards = 50, our_turns_total = 15, opp_turns_total = 15,
Thrower_gender = 'MMP', Thrower_role = 'handler', Thrower_line = 'dline', gender_ratio = '4MM', line = 'dline')

# Predicted scoring probability in simple regression
ex_a_prediction <- predict(simple_epa_logistic, newdata = ex_a, type = "response")
ex_b_prediction <- predict(simple_epa_logistic, newdata = ex_b, type = "response")
ex_c_prediction <- predict(simple_epa_logistic, newdata = ex_c, type = "response")
ex_d_prediction <- predict(simple_epa_logistic, newdata = ex_d, type = "response")

# Predicted scoring probabily in complex regression
ex_a_prediction_complex <- predict(full_ep_logistic_model, newdata = ex_a, type = "response")
ex_b_prediction_complex <- predict(full_ep_logistic_model, newdata = ex_b, type = "response")
ex_c_prediction_complex <- predict(full_ep_logistic_model, newdata = ex_c, type = "response")
ex_d_prediction_complex <- predict(full_ep_logistic_model, newdata = ex_d, type = "response")


# 1. Create the data frame 
stock_comparison <- tribble(
  ~Situation, ~Simple_EP, ~Complex_EP,
  "A: WMP O-line handler at brick mark", round(ex_a_prediction, digits = 2), round(ex_a_prediction_complex, digits = 2),
  "B: MMP O-line cutter at scoring endzone line", round(ex_b_prediction, digits = 2), round(ex_b_prediction_complex, digits = 2),
  "C: WMP D-line cutter at coffin corner (far endzone)", round(ex_c_prediction, digits = 2), round(ex_c_prediction_complex, digits = 2),
  "D: MMP D-line handler in middle of field", round(ex_d_prediction, digits = 2), round(ex_d_prediction_complex, digits = 2)
)

# 2. Build the gt table
stock_comparison %>%
  gt() %>%
  tab_header(
    title = "Table 3: Comparison of Stock Scoring Probabilities",
    subtitle = "Simple Model (Distance only) vs. Complex Model (Situational Context)"
  ) %>%
  # Rename columns for clarity
  cols_label(
    Situation = "Scenario Description",
    Simple_EP = "Simple Model EP",
    Complex_EP = "Complex Model EP"
  ) %>%
  # Highlight the columns
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  # Add a source note or explanation of the shift
  tab_source_note(
    source_note = "Note the significant drop in EP for Situation C and D when 
                   the model accounts for the possession being on the D-line."
  ) 
```

#### Next Steps

In the next post I'll refine this EP model and then extend it into the EPA model which originally motivated our work. Later I want to apply this model to UFA data to explore whether its robust to high level Open play and team to team differences.